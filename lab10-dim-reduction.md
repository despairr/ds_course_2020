# Лабораторная работа №10: Понижение размерности признакового пространства

- [статья про кластеризацию и порнижение размерности от ODS](https://habr.com/ru/company/ods/blog/325654/)
- [материалы про понижение размерности из курса Яндекс/МФТИ на coursera](https://www.coursera.org/learn/unsupervised-learning/home/week/2)
- [методичка про понижение размерности из курса Яндекс/МФТИ](https://github.com/Tirren/data-analysis/blob/master/Machine%20learning/coursera/Notes/3%20%D0%BA%D1%83%D1%80%D1%81/2-1.Ponizhenie_razmernosti_i_otbor_priznakov.pdf)

0. Найти датасет с большим количеством вещественных признаков (от 20-ти шт.)

1. Произвести визуализацию точек из исходного признакового пространства в 2-мерном пространстве, используя алгоритм t-SNE (`sklearn.manifold.TSNE`)

2. Произвести визуализацию точек из исходного признакового пространства в 2-мерном пространстве, используя метод главных компонент (`sklearn.decomposition.PCA`) или Truncated SVD (`sklearn.decomposition.TruncatedSVD`)

3. На практике, как правило, выбирают столько главных компонент, чтобы оставить 90% дисперсии исходных данных. Чтобы оценить необходимое количество компонент, постройте график *Number of components* / *Total explained variance*. Пример построения есть в статье от ODS.

4. Возьмите модель машинного обучения из любой предыдущей работы (главное, чтобы в ней было минимум несколько вещественных признаков), либо постройте новую. Сравните метрики качества исходной модели и другой модели, где вещественные признаки предварительно прошли через процедуру снижения размерности. Напишите выводы.  
PS: если исходные признаки были сильно кореллированы и вы правильно произвели процедуру снижения размерности, оставив "правильное" число главных компонент, не потеряв существенной информации при этом, качество некоторых моделей может вырасти. Но в данной работе не требуется, чтобы непременно был выигрыш.