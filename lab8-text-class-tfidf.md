# Лабораторная работа №8: Классификация текстов (с помощью TfIdf + LogisticRegression)

Это первое задание, где мы будем осознанно добиваться наилучшей точности предсказаний и где об этом нужно всерьёз задумываться в процессе исполнения.

"Заготовка" для выполнения ЛР [тут](tfidf-example.ipynb). В этом блокноте также перечислены полезные статьи на тему классификации текстов с помощью мешка слов / TfIdf.

1. Найти и загрузить любой датасет для классификации текстов (бинарная или многоклассовая). Для примера можете тренироваться с датасетом [billdoard_dataset.csv](https://github.com/Yorko/mlcourse.ai/blob/master/data/billdoard_dataset.csv) (который я использую в заготовке tfidf-example.ipynb), но потом нужно найти собстивенные датасеты и переделавать под них.  
Если же говорить о датасете billdoard_dataset.csv, то там мы можем свести задачу к бинарной классификации, прогнозировать класс популярности объявления (если num_counts выше медианы — один класс, иначе другой).

2. Выводите в цикле целиком текст первых нескольких записей (этот этап естественно также должен быть отражён в блокноте), смотрите на тексты глазами, много думаете.

3. Находите малоинформативные повторяющиеся участки в текстах:
это могут быть, например, стандартные «шапки» или стандартные «подвалы» (стандартные приветствия или подписи), или, например, все цифры (если в данном контексте они малоинформативны) или все MD5-хеши, или web-адреса и т.п. — всё что по вашей оценке не несёт информационной нагрузки и пользы для классификации — всё это удаляем (с помощью регекспов или как-то ещё через функции работы со строками). Чтобы не зашумлять тексты малоинфомативными мусорными n-граммами.

4. Опциональный шаг: Проходимся по текстам стеммером (пример стемминга есть в tfidf-example.ipynb: если PyStemmer стеммер не заработает, найдите другие на просторах интернета и поделитесь в общем чате вашими рабочими вариантами) — приводим все слова к нормальной форме. Все результаты должны быть в двух вариантах: без стеммера и со стеммером.  
Можно использовать не стеммер, а лемматизатор (например, `pymorph2`)

5. Создаём матрицу TfIdf с помощью `TfidfVectorizer` (пример есть в моём блокноте), не забудьте про стоп-слова (`stop_words`), наряду со словами из библиотеки stop_words можете добавить свои стоп-слова, если по текстам видите, что они мусорные и незначимые.  
Вначале при инициализации `TfidfVectorizer` можете оставить значения по умолчанию, которые даны в моём примере.

6. Применяем `sklearn.linear_model.LogisticRegression()` (либо другой классификатор по вашему выбору) к получившейся матрице TfIdf, проводим классификацию (здесь и далее всё с помощью кросс-валидации!), выводим получившиеся метрики качества (accuracy для multiclass или f1 / `classification_report` для бинарной).

7. Теперь самое интересное: оптимизация и подбор наилучших гиперпатаметров и способа предобработки текстов.  
Играемся с параметрами `TfidfVectorizer`: `max_df`, `min_df`, `max_features`, `ngram_range` ((1,2) или (1,3)), чтобы максимально повысить качество предсказаний (в качестве метрики качества для оптимизации: accuracy для многоклассовой и f1 для бинарной классификации).  
В идеале если вы сможете всё это завернуть в свой `Pipeline` и подобрать лучшие гиперпараметры пайплайна через `GridSearchCV`.  
Помимо указанных параметров `TfidfVectorizer` также попробуйте несколько разных значений силы регуляризации: `LogisticRegression`: `C = [ 0.1, 1, 10 ]`.  
И не забудьте что всё это ещё надо попробовать со стеммером и без: наличие фазы стемминга тоже, считайте, гиперпатаметр.
Если повезёт, то все гиперпараметры вы можете засунуть в Pipeline и найти наилучшее их сочетание через `GridSearhCV`, в худшем случае что-то придётся перебирать в цикле (но, по крайней мере, частично надо автоматизировать через `GridSearhCV`).  

    В самом худшем случае, если всё совсем плохо и с автоматизацией не получается совсем, можете подобрать / покрутить все параметры ручками, но тогда запишите куда-то промежуточные результаты, что у вас получались при каком сочетании гиперпараметров.

8. Подводим итоги: параметры наилучшей модели и получившиеся при них метрики классификации.

9. Строим [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) (см. ЛР4) для оценки того, какие ошибки классификатор допускает наиболее часто.

Лучше всего всё оптимизируем и проверяем через кросс-валидацию (см. [как совместить CV и classification_report](https://stackoverflow.com/questions/42562146/classification-report-with-nested-cross-validation-in-sklearn)), но в худшем случае можете считать метрики на отложенной выборке и / или вместо `classification_report` использовать просто accuracy / f1, но за это будет немного снижен балл.

Классификатор `LogisticRegression` может быть заменён по вашему желанию на другой адекватный для данной задачи классификатор, а ещё лучше дополнен другим классификатором, чтобы было 2 разных классификатора для сравнения на одних и тех же данных. В последнем случае: +1 балл за задачу.
